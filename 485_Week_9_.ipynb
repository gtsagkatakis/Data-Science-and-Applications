{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gtsagkatakis/Data-Science-and-Applications/blob/main/485_Week_9_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Transformers\n",
        "Transformers have been widely used in natural language processing tasks, such as language translation and text generation, as well as in various other machine learning applications.\n",
        "\n",
        "In simple terms, a transformer architecture is a type of deep learning model designed to process sequences of data, such as text or time series, by considering ***all the input data at once rather than processing it sequentially***.\n",
        "\n",
        "![](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Transformers/blob/master/img/access.PNG?raw=true)\n",
        "\n",
        "It uses **attention mechanisms** to weigh the importance of different parts of the input, allowing it to capture complex relationships within the data more effectively.\n"
      ],
      "metadata": {
        "id": "0EhO4rJajsh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Attention?\n",
        "\n",
        "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. There are a lot of different possible definitions of \"attention\" in the literature, but the one we will use here is the following:\n",
        "\n",
        "_the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically_.\n",
        "\n",
        "So what does this exactly mean? The goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to \"attend\" more than others. In particular, an attention mechanism has usually four parts we need to specify:\n",
        "\n",
        "* **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
        "* **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is \"offering\", or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
        "* **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
        "* **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
        "\n",
        "Visually, we can show the attention over a sequence of words as follows:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/attention_example.svg?raw=1\" width=\"750px\"></center>\n",
        "\n",
        "The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query. If we try to describe it with pseudo-math, we can write:\n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
        "$$\n",
        "\n",
        "\n",
        "For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights.\n",
        "\n",
        "- Most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used.\n",
        "\n",
        "The attention applied inside the Transformer architecture is called **self-attention**. In self-attention, for each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements' keys, and returned a different, averaged value vector for each element."
      ],
      "metadata": {
        "id": "BTdI86IalKHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaled Dot Product Attention\n",
        "\n",
        "The core concept behind self-attention is the scaled dot product attention. Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries $Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$ and values $V\\in\\mathbb{R}^{T\\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. In math, we calculate the dot product attention as follows:\n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "The matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T\\times T$. Each row represents the attention logits for a specific element $i$ to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/scaled_dot_product_attn.svg?raw=1\" width=\"210px\"></center>\n",
        "\n",
        "One aspect we haven't discussed yet is the scaling factor of $1/\\sqrt{d_k}$. This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. Remember that we intialize our layers with the intention of having equal variance throughout the model, and hence, $Q$ and $K$ might also have a variance close to $1$. However, performing a dot product over two vectors with a variance $\\sigma^2$ results in a scalar having $d_k$-times higher variance:\n",
        "\n",
        "$$q_i \\sim \\mathcal{N}(0,\\sigma^2), k_i \\sim \\mathcal{N}(0,\\sigma^2) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma^4\\cdot d_k$$\n",
        "\n",
        "\n",
        "If we do not scale down the variance back to $\\sim\\sigma^2$, the softmax over the logits will already saturate to $1$ for one random element and $0$ for all others. The gradients through the softmax will be close to zero so that we can't learn the parameters appropriately. Note that the extra factor of $\\sigma^2$, i.e., having $\\sigma^4$ instead of $\\sigma^2$, is usually not an issue, since we keep the original variance $\\sigma^2$ close to $1$ anyways.\n",
        "\n",
        "The block Mask (opt.) in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value."
      ],
      "metadata": {
        "id": "GxR9XlrEmq5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The scaled dot product attention allows a network to attend over a sequence.\n",
        "However, often there are multiple different aspects a sequence element wants to attend to,\n",
        "and a single weighted average is not a good option for it.\n",
        "This is why we extend the attention mechanisms to multiple heads,\n",
        "i.e. multiple different query-key-value triplets on the same features.\n",
        "Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys,\n",
        "and sub-values, which we pass through the scaled dot product attention independently.\n",
        "Afterward, we concatenate the heads and combine them with a final weight matrix.\n",
        "Mathematically, we can express this operation as:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
        "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "We refer to this as Multi-Head Attention layer with the learnable parameters\n",
        "$W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$,\n",
        "$W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$,\n",
        "$W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$,\n",
        "and $W^{O}\\in\\mathbb{R}^{h\\cdot d_k\\times d_{out}}$ ($D$ being the input dimensionality).\n",
        "Expressed in a computational graph, we can visualize it as below:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/multihead_attention.svg?raw=1\" width=\"230px\"></center>\n",
        "\n",
        "How are we applying a Multi-Head Attention layer in a neural network,\n",
        "where we don't have an arbitrary query, key, and value vector as input?\n",
        "Looking at the computation graph above, a simple but effective implementation is to set the current\n",
        "feature map in a NN, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$\n",
        "($B$ being the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$).\n",
        "The consecutive weight matrices $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding\n",
        "feature vectors that represent the queries, keys, and values of the input.\n",
        "Using this approach, we can implement the Multi-Head Attention module below."
      ],
      "metadata": {
        "id": "EomUGwJro5k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positional Encoding\n",
        "\n",
        "In order for the model to make sense of the sentence, it needs to know two things about the each word.\n",
        "\n",
        "- what is the position of the word in the sentence.\n",
        "\n",
        "In \"attention is all you need paper\" author used the following functions to create positional encoding. On odd time steps a cosine function is used and in even time steps a sine function is used.\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://miro.medium.com/max/524/1*yWGV9ck-0ltfV2wscUeo7Q.png\" width=\"230px\"></center>\n",
        "\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://miro.medium.com/max/564/1*SgNlyFaHH8ljBbpCupDhSQ.png\" width=\"230px\"></center>\n",
        "\n",
        "where pos refers to order in the sentence and i refers to position along embedding vector dimension.\n",
        "\n",
        "Positinal embedding will generate a matrix of similar to embedding matrix. It will create a matrix of dimension sequence length x embedding dimension.\n",
        "\n",
        "for eg: if we have batch size of 32 and seq length of 10 and let embedding dimension be 512. Then we will have embedding vector of dimension 32 x 10 x 512. Similarly we will have positional encoding vector of dimension 32 x 10 x 512. Then we add both."
      ],
      "metadata": {
        "id": "1UGhksCXuan9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Natural Language Processing with Transformers\n",
        "Create a Translation Machine with Transformers to translate French to English\n",
        "\n"
      ],
      "metadata": {
        "id": "zhi6Uf08tzb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ],
      "metadata": {
        "id": "ZUNc-us4AMg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the model and the required functions"
      ],
      "metadata": {
        "id": "Kgyb6UR5AgAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch():\n",
        "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
        "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
        "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
        "    return torch.LongTensor(input_batch), torch.LongTensor(output_batch), torch.LongTensor(target_batch)\n",
        "\n",
        "def get_sinusoid_encoding_table(n_position, d_model):\n",
        "    def cal_angle(position, hid_idx):\n",
        "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "    return torch.FloatTensor(sinusoid_table)\n",
        "\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    # print(seq_q)\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
        "\n",
        "# create a mask that prevents positions in the sequence from attending to subsequent positions during self-attention\n",
        "def get_attn_subsequent_mask(seq):\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
        "    return subsequent_mask\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "        self.linear = nn.Linear(n_heads * d_v, d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = self.linear(context)\n",
        "        return self.layer_norm(output + residual), attn # output: [batch_size x len_q x d_model]\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        residual = inputs # inputs : [batch_size, len_q, d_model]\n",
        "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        return self.layer_norm(output + residual)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dec_self_attn = MultiHeadAttention()\n",
        "        self.dec_enc_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
        "        dec_outputs = self.pos_ffn(dec_outputs)\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True)\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n",
        "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "        enc_self_attns = []\n",
        "        for layer in self.layers:\n",
        "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
        "            enc_self_attns.append(enc_self_attn)\n",
        "        return enc_outputs, enc_self_attns\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model),freeze=True)\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n",
        "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n",
        "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
        "\n",
        "        dec_self_attns, dec_enc_attns = [], []\n",
        "        for layer in self.layers:\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "            dec_self_attns.append(dec_self_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)  # encodes the input sequence\n",
        "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)   # decodes it into the output sequence\n",
        "        dec_logits = self.projection(dec_outputs) #   # projects the output into the target vocabulary space (dec_logits : [batch_size x src_vocab_size x tgt_vocab_size])\n",
        "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n"
      ],
      "metadata": {
        "id": "lnmxz1JSAMYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the model"
      ],
      "metadata": {
        "id": "XAFjycVvAcU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "\n",
        "sentences = ['je veux une biere P', 'S i want a beer', 'i want a beer E']\n",
        "\n",
        "# Tokenization and Embeddings\n",
        "# Padding --> 0\n",
        "src_vocab = {'P': 0, 'je': 1, 'veux': 2, 'une': 3, 'biere': 4}\n",
        "src_vocab_size = len(src_vocab)\n",
        "\n",
        "tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'S': 5, 'E': 6}\n",
        "number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "src_len = 5 # length of source\n",
        "tgt_len = 5 # length of target\n",
        "\n",
        "# Transformer Parameters\n",
        "d_model = 512  # Embedding Size\n",
        "d_ff = 2048  # FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_layers = 6  # number of Encoder of Decoder Layer\n",
        "n_heads = 8  # number of heads in Multi-Head Attention\n",
        "\n",
        "model = Transformer()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "enc_inputs, dec_inputs, target_batch = make_batch()\n",
        "\n",
        "for epoch in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
        "    loss = criterion(outputs, target_batch.contiguous().view(-1))\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJTsxKSkAMPh",
        "outputId": "cab2ef5f-cf31-439b-ed32-b1a620a679f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss = 2.328215\n",
            "Epoch: 0002 loss = 1.517787\n",
            "Epoch: 0003 loss = 3.382354\n",
            "Epoch: 0004 loss = 3.740348\n",
            "Epoch: 0005 loss = 5.650603\n",
            "Epoch: 0006 loss = 6.138568\n",
            "Epoch: 0007 loss = 6.812965\n",
            "Epoch: 0008 loss = 4.507048\n",
            "Epoch: 0009 loss = 2.358359\n",
            "Epoch: 0010 loss = 3.526624\n",
            "Epoch: 0011 loss = 1.753000\n",
            "Epoch: 0012 loss = 1.847169\n",
            "Epoch: 0013 loss = 1.814213\n",
            "Epoch: 0014 loss = 1.853084\n",
            "Epoch: 0015 loss = 1.850468\n",
            "Epoch: 0016 loss = 1.746461\n",
            "Epoch: 0017 loss = 1.643406\n",
            "Epoch: 0018 loss = 1.617053\n",
            "Epoch: 0019 loss = 1.650090\n",
            "Epoch: 0020 loss = 1.691395\n",
            "Epoch: 0021 loss = 1.708117\n",
            "Epoch: 0022 loss = 1.697560\n",
            "Epoch: 0023 loss = 1.674343\n",
            "Epoch: 0024 loss = 1.652054\n",
            "Epoch: 0025 loss = 1.634558\n",
            "Epoch: 0026 loss = 1.620934\n",
            "Epoch: 0027 loss = 1.612200\n",
            "Epoch: 0028 loss = 1.611247\n",
            "Epoch: 0029 loss = 1.618249\n",
            "Epoch: 0030 loss = 1.627668\n",
            "Epoch: 0031 loss = 1.631572\n",
            "Epoch: 0032 loss = 1.625930\n",
            "Epoch: 0033 loss = 1.613366\n",
            "Epoch: 0034 loss = 1.600493\n",
            "Epoch: 0035 loss = 1.592769\n",
            "Epoch: 0036 loss = 1.591538\n",
            "Epoch: 0037 loss = 1.594587\n",
            "Epoch: 0038 loss = 1.598534\n",
            "Epoch: 0039 loss = 1.600705\n",
            "Epoch: 0040 loss = 1.599836\n",
            "Epoch: 0041 loss = 1.595937\n",
            "Epoch: 0042 loss = 1.589975\n",
            "Epoch: 0043 loss = 1.583391\n",
            "Epoch: 0044 loss = 1.577730\n",
            "Epoch: 0045 loss = 1.574084\n",
            "Epoch: 0046 loss = 1.572492\n",
            "Epoch: 0047 loss = 1.571844\n",
            "Epoch: 0048 loss = 1.570368\n",
            "Epoch: 0049 loss = 1.566698\n",
            "Epoch: 0050 loss = 1.560594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evalute the mode (translate)"
      ],
      "metadata": {
        "id": "xoA44c02Az0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
        "predict = predict.data.max(1, keepdim=True)[1]\n",
        "print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSJQ6jl5AMDw",
        "outputId": "e715e57e-d314-4345-f1f6-c3cb2af25175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je veux une biere P -> ['i', 'beer', 'beer', 'beer', 'beer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Another practical example of Transformers for classification\n",
        "The goal here is to perform sentiment analysis using Transformers.\n",
        "This example utilizes the **Hugging Face Transformers library**, which provides pre-trained transformer models and easy-to-use interfaces for natural language processing tasks like sentiment analysis."
      ],
      "metadata": {
        "id": "6ZHyDXM97sf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "1sDGB4K96mFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the model"
      ],
      "metadata": {
        "id": "HtttsP52sqX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_seq_len, embed_size):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.positional_encoding = torch.zeros(max_seq_len, embed_size)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
        "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.positional_encoding = self.positional_encoding.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.positional_encoding[:x.size(0), :]\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, max_seq_len, embed_size, num_heads, num_layers):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(max_seq_len, embed_size)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(embed_size, num_heads, hidden_size)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(embed_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = x.permute(1, 0, 2)  # Change the sequence length to the first dimension\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = torch.mean(x, dim=0)  # Average pooling\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "OD8nO1fuspPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the data with pre-trained tokenizer"
      ],
      "metadata": {
        "id": "ug17uyfGswHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming you have your data in the form of sentences and corresponding labels\n",
        "sentences = [\"This movie is great!\", \"I hated the food.\", \"The performance was average.\"]\n",
        "labels = [1, 0, 1]  # Assuming 1 is positive sentiment and 0 is negative sentiment\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize input sentences\n",
        "train_inputs = tokenizer(train_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "test_inputs = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Create DataLoader for train and test sets\n",
        "train_data = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
        "test_data = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "oRvg8jGLsv6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the model"
      ],
      "metadata": {
        "id": "EhqemG2sskBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = tokenizer.vocab_size\n",
        "hidden_size = 100\n",
        "output_size = 2  # Binary classification: positive or negative sentiment\n",
        "max_seq_len = max(len(train_inputs['input_ids'][0]), len(test_inputs['input_ids'][0]))\n",
        "embed_size = 100  # Embedding size, can be adjusted\n",
        "num_heads = 2\n",
        "num_layers = 2\n",
        "model = SentimentClassifier(input_size, hidden_size, output_size, max_seq_len, embed_size, num_heads, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, attention_mask, labels in train_loader:\n",
        "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cCrLqDesjFR",
        "outputId": "9667056d-bd39-4535-b568-eabd0ea41a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.7727504372596741\n",
            "Epoch 2/10, Loss: 0.6107921600341797\n",
            "Epoch 3/10, Loss: 0.46461033821105957\n",
            "Epoch 4/10, Loss: 0.28331124782562256\n",
            "Epoch 5/10, Loss: 0.13429999351501465\n",
            "Epoch 6/10, Loss: 0.07085411250591278\n",
            "Epoch 7/10, Loss: 0.04557619243860245\n",
            "Epoch 8/10, Loss: 0.0174559373408556\n",
            "Epoch 9/10, Loss: 0.01221940852701664\n",
            "Epoch 10/10, Loss: 0.006254475563764572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "sT7tPJlStYev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, attention_mask, labels in test_loader:\n",
        "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        # print(predicted)\n",
        "        total += labels.size(0)\n",
        "        # print(labels)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv--mQLZ8B9j",
        "outputId": "3917ea95-940c-4375-c8f5-f1472fd837d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "tensor([1])\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 8"
      ],
      "metadata": {
        "id": "KfpSC68ntclw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create your own translation machine with transformers: From English to Italian"
      ],
      "metadata": {
        "id": "OrPMe0qUuvZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence to translate"
      ],
      "metadata": {
        "id": "pwTMwAueSr4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['i love this course P', 'S adoro questo corso', 'adoro questo corso E']"
      ],
      "metadata": {
        "id": "QYt7p6R5TpLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform tokenization and padding"
      ],
      "metadata": {
        "id": "4GAOn_XBTjW1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTgBebjQc49N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the model and required functions"
      ],
      "metadata": {
        "id": "RiNfNdsLT1vv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ijteGghod8yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perform hyperparameter tuning\n",
        "\n",
        "- Save the model with the lowest loss"
      ],
      "metadata": {
        "id": "MhE1HDSQUARv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_3254l-d8v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the best model to evaluate your model"
      ],
      "metadata": {
        "id": "Sd6-pjh0UFGg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MQXlPGy5d8s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize these attention weights\n",
        "\n",
        "- The attention weights of the first head of the self-attention mechanism in the last state of the encoder.\n",
        "\n",
        "- The attention weights of the first head of the self-attention mechanism in the last state of the decoder.\n",
        "\n",
        "- The attention weights of the first head of the decoder-encoder attention mechanism in the last state of the decoder."
      ],
      "metadata": {
        "id": "Is8d7rfhUZJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(attn):\n",
        "    attn = attn[-1].squeeze(0)[0]\n",
        "    attn = attn.squeeze(0).data.numpy()\n",
        "    fig = plt.figure(figsize=(n_heads, n_heads))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attn, cmap='viridis')\n",
        "    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n",
        "    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0o4Lk4QbeiOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}